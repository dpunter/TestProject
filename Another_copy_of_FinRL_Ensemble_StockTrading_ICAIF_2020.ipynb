{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpunter/TestProject/blob/main/Another_copy_of_FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXaoZs2lh1hi"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
        "\n",
        "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
        "\n",
        "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
        "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
        "* **Pytorch Version**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGunVt8oLCVS"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOzAKQ-SLGX6"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sApkDlD9LIZv"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLD2TZSLKZ-"
      },
      "source": [
        "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
        "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
        "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsre789LY08"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPT0ipYE28wL"
      },
      "outputs": [],
      "source": [
        "# ## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBHhVysOEzi"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPqeTTwoh1hn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2owTj985RW4"
      },
      "source": [
        "<a id='1.4'></a>\n",
        "## 2.4. Create Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9A8CN5R5PuZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR,\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        "    TRAIN_START_DATE,\n",
        "    TRAIN_END_DATE,\n",
        "    TEST_START_DATE,\n",
        "    TEST_END_DATE,\n",
        "    TRADE_START_DATE,\n",
        "    TRADE_END_DATE,\n",
        ")\n",
        "\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A289rQWMh1hq"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeQ7iS-LoMm"
      },
      "source": [
        "\n",
        "\n",
        "-----\n",
        "class YahooDownloader:\n",
        "    Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzqRRTOX6aFu"
      },
      "outputs": [],
      "source": [
        "print(DOW_30_TICKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCKm4om-s9kE",
        "outputId": "456bbe0c-bff0-4804-bf34-5dbdcf21e1cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of DataFrame:  (97013, 8)\n"
          ]
        }
      ],
      "source": [
        "# TRAIN_START_DATE = '2009-04-01'\n",
        "# TRAIN_END_DATE = '2021-01-01'\n",
        "# TEST_START_DATE = '2021-01-01'\n",
        "# TEST_END_DATE = '2022-06-01'\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "\n",
        "TRAIN_START_DATE = '2010-01-01'\n",
        "TRAIN_END_DATE = '2021-10-01'\n",
        "TEST_START_DATE = '2021-10-01'\n",
        "TEST_END_DATE = '2023-03-01'\n",
        "\n",
        "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
        "                     end_date = TEST_END_DATE,\n",
        "                     ticker_list = DOW_30_TICKER).fetch_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM5bH9uroCeg"
      },
      "outputs": [],
      "source": [
        "#  INDICATORS = ['macd',\n",
        "#                'rsi_30',\n",
        "#                'cci_30',\n",
        "#                'dx_30']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgXfBcjxtj1a",
        "outputId": "44e0f2de-804c-4b54-ff20-f0e3aeeb50d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n"
          ]
        }
      ],
      "source": [
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "fe = FeatureEngineer(use_technical_indicator=True,\n",
        "                     tech_indicator_list = INDICATORS,\n",
        "                     use_turbulence=True,\n",
        "                     user_defined_feature = False)\n",
        "\n",
        "processed = fe.preprocess_data(df)\n",
        "processed = processed.copy()\n",
        "processed = processed.fillna(0)\n",
        "processed = processed.replace(np.inf,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiFA3pRWPQMO"
      },
      "outputs": [],
      "source": [
        "processed.to_csv(\"processed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
        "\n",
        "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zqII8rMIqn",
        "outputId": "ed28674c-6b5e-46ac-c29d-5f9f3463bd81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(processed.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWyp84Ltto19"
      },
      "outputs": [],
      "source": [
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"buy_cost_pct\": 0.001,\n",
        "    \"sell_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"print_verbosity\":5\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
        "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
        "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms.\n",
        "\n",
        "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-gthCxMtj1d"
      },
      "outputs": [],
      "source": [
        "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
        "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
        "\n",
        "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
        "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
        "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
        "                 rebalance_window=rebalance_window,\n",
        "                 validation_window=validation_window,\n",
        "                 **env_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsfEHa_Etj1d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.005,\n",
        "                    'learning_rate': 0.0007\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.01,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00025,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
        "                      \"buffer_size\": 10_000,\n",
        "                      \"learning_rate\": 0.0005,\n",
        "                      \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "SAC_model_kwargs = {\n",
        "    \"batch_size\": 64,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "TD3_model_kwargs = {\"batch_size\": 100, \"buffer_size\": 1000000, \"learning_rate\": 0.0001}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 10_000,\n",
        "                 'ppo' : 10_000,\n",
        "                 'ddpg' : 10_000,\n",
        "                 'sac' : 10_000,\n",
        "                 'td3' : 10_000\n",
        "                 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1lyCECstj1e",
        "scrolled": true,
        "outputId": "aa37eebe-74b1-4020-98f6-e413deb4ba00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  201.7422519635256\n",
            "======Model training from:  2010-01-01 to  2021-10-04\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 96         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -20.3      |\n",
            "|    reward             | 0.23653331 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 0.451      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 90        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 34.8      |\n",
            "|    reward             | 0.5499224 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 3.11      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.0373    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -39.1      |\n",
            "|    reward             | -3.7539508 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 5.53       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 107       |\n",
            "|    reward             | 2.4693222 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 14        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 89         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 27         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.0388    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -261       |\n",
            "|    reward             | -1.6224284 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 45.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 90        |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -5.86     |\n",
            "|    reward             | 1.2813525 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.128     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 88          |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 39          |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0.305       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | 6.28        |\n",
            "|    reward             | 0.055486437 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 0.222       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 89         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 44         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.0987     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 104        |\n",
            "|    reward             | -0.4804503 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 7.73       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 88          |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 50          |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -194        |\n",
            "|    reward             | -0.18002136 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 31.6        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 89         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 56         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.000239   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -551       |\n",
            "|    reward             | 0.95738435 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 302        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 86        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 63        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.0455   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -282      |\n",
            "|    reward             | 3.1043186 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 86.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 85         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 70         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0.139      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -58.3      |\n",
            "|    reward             | -0.6711612 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 2.79       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 76          |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41         |\n",
            "|    explained_variance | 0.0938      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 36.8        |\n",
            "|    reward             | -0.27128124 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 1.36        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 81          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 20.4        |\n",
            "|    reward             | -0.96694565 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 2.28        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 86         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 87         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 98.7       |\n",
            "|    reward             | 0.59351486 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 7.07       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 86       |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 93       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 8.43     |\n",
            "|    reward             | 5.229751 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 41.2     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 86         |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 98         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 390        |\n",
            "|    reward             | -3.1343346 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 266        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 86        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 104       |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.287    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -46.7     |\n",
            "|    reward             | 0.2947359 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 2.68      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 86        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 109       |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 26.5      |\n",
            "|    reward             | 0.9860132 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.62      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 86        |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 115       |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 22.6      |\n",
            "|    reward             | 1.7295032 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.71      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2021-10-04 to  2022-01-03\n",
            "a2c Sharpe Ratio:  0.2838871959260896\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_2\n",
            "day: 2957, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4816941.69\n",
            "total_reward: 3816941.69\n",
            "total_cost: 5419.83\n",
            "total_trades: 65474\n",
            "Sharpe: 0.791\n",
            "=================================\n",
            "======ddpg Validation from:  2021-10-04 to  2022-01-03\n",
            "ddpg Sharpe Ratio:  0.11436820213506049\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 2957, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3762803.69\n",
            "total_reward: 2762803.69\n",
            "total_cost: 1415.14\n",
            "total_trades: 55598\n",
            "Sharpe: 0.785\n",
            "=================================\n",
            "======td3 Validation from:  2021-10-04 to  2022-01-03\n",
            "td3 Sharpe Ratio:  0.4087991145030786\n",
            "======sac Training========\n",
            "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 2957, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5153069.75\n",
            "total_reward: 4153069.75\n",
            "total_cost: 282999.68\n",
            "total_trades: 77402\n",
            "Sharpe: 0.876\n",
            "=================================\n",
            "======sac Validation from:  2021-10-04 to  2022-01-03\n",
            "sac Sharpe Ratio:  0.1480390831020574\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 96        |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 21        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.9107347 |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 94         |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 43         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01649544 |\n",
            "|    clip_fraction        | 0.217      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.2      |\n",
            "|    explained_variance   | -0.0421    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4.02       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0287    |\n",
            "|    reward               | 0.65386444 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 12.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 94          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 64          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018022425 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00465    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.2        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    reward               | -0.14343806 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 46          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 94          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015256576 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00231     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.1        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    reward               | 3.2759042   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 38.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 93          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 109         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019090721 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.022       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.88        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0264     |\n",
            "|    reward               | 0.4014322   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2021-10-04 to  2022-01-03\n",
            "ppo Sharpe Ratio:  0.19097317983634857\n",
            "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
            "======Trading from:  2022-01-03 to  2022-04-04\n",
            "============================================\n",
            "turbulence_threshold:  201.7422519635256\n",
            "======Model training from:  2010-01-01 to  2022-01-03\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 80         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -101       |\n",
            "|    reward             | 0.20287752 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 7.35       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 87        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 2.6       |\n",
            "|    reward             | 0.9622804 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.53      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 85         |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0395     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -0.551     |\n",
            "|    reward             | -3.1933916 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.45       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 87        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.0496   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 95.3      |\n",
            "|    reward             | -0.697296 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 12.3      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 86         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -118       |\n",
            "|    reward             | 0.11775666 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 9          |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 88        |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.155    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 67.7      |\n",
            "|    reward             | 3.6429439 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 8.01      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 88         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.189     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -136       |\n",
            "|    reward             | -0.5420297 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 12         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 88        |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.00345   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 57.9      |\n",
            "|    reward             | 0.9948733 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 2.7       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 89         |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -87.7      |\n",
            "|    reward             | -0.7978353 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 8.12       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 88         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 56         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 12.2       |\n",
            "|    reward             | -0.8295953 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 0.724      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 88        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 61        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -128      |\n",
            "|    reward             | 3.2653763 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 12.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 88         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 67         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 131        |\n",
            "|    reward             | -2.1107533 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 18         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 88        |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 73        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.161     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 55.7      |\n",
            "|    reward             | 2.3311265 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.26      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 86         |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 80         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 112        |\n",
            "|    reward             | -0.8113228 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 9.63       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 84        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 88        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -4.77e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 62.8      |\n",
            "|    reward             | 0.3420644 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 3.36      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 84         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 95         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.436     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 152        |\n",
            "|    reward             | -0.9914484 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 17.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 84        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -343      |\n",
            "|    reward             | 2.0774937 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 73.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 84        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 106       |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.219    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 263       |\n",
            "|    reward             | -3.765514 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 49.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 85        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 111       |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.031    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 46.3      |\n",
            "|    reward             | 1.9391553 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.26      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 84         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 117        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.113      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 43.5       |\n",
            "|    reward             | -1.6818924 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.84       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2022-01-03 to  2022-04-04\n",
            "a2c Sharpe Ratio:  -0.03111315123602695\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "day: 3020, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4484000.06\n",
            "total_reward: 3484000.06\n",
            "total_cost: 6282.46\n",
            "total_trades: 43577\n",
            "Sharpe: 0.735\n",
            "=================================\n",
            "======ddpg Validation from:  2022-01-03 to  2022-04-04\n",
            "ddpg Sharpe Ratio:  -0.15099324759604432\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "day: 3020, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6098616.62\n",
            "total_reward: 5098616.62\n",
            "total_cost: 1130.76\n",
            "total_trades: 45290\n",
            "Sharpe: 0.930\n",
            "=================================\n",
            "======td3 Validation from:  2022-01-03 to  2022-04-04\n",
            "td3 Sharpe Ratio:  -0.3072368238518503\n",
            "======sac Training========\n",
            "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "day: 3020, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3780529.94\n",
            "total_reward: 2780529.94\n",
            "total_cost: 232054.93\n",
            "total_trades: 77290\n",
            "Sharpe: 0.617\n",
            "=================================\n",
            "======sac Validation from:  2022-01-03 to  2022-04-04\n",
            "sac Sharpe Ratio:  -0.16892556757738778\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 95        |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 21        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1.1696498 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 92          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 44          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013612773 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0139     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.21        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0246     |\n",
            "|    reward               | -1.7981759  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 93          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 66          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016962556 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.0163      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13          |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0238     |\n",
            "|    reward               | -0.779104   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 46.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 92          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 88          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014699316 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0163     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.3        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    reward               | -0.7774461  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 55.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 92          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 110         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021943629 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00277     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.58        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0221     |\n",
            "|    reward               | 1.2392203   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 19          |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2022-01-03 to  2022-04-04\n",
            "ppo Sharpe Ratio:  -0.30652017820975125\n",
            "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
            "======Trading from:  2022-04-04 to  2022-07-06\n",
            "============================================\n",
            "turbulence_threshold:  201.7422519635256\n",
            "======Model training from:  2010-01-01 to  2022-04-04\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 84        |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -117      |\n",
            "|    reward             | 0.0451019 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 10.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 85       |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -39.4    |\n",
            "|    reward             | 1.13708  |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 3.19     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 87         |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0969    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -50.6      |\n",
            "|    reward             | -3.6922534 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.98       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 0.0147      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -48.7       |\n",
            "|    reward             | -0.22032951 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.97        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 87         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -144       |\n",
            "|    reward             | -0.3227339 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 13.8       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 83       |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 12.4     |\n",
            "|    reward             | 8.590185 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.98     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 85        |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.0183    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -262      |\n",
            "|    reward             | 2.0318205 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 49.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 84         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 47         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -99.3      |\n",
            "|    reward             | 0.47762534 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 6.53       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 52          |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -39         |\n",
            "|    reward             | -0.96476054 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 2.55        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 84        |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 59        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 115       |\n",
            "|    reward             | 4.7667537 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 9.49      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 85         |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 64         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0102    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -445       |\n",
            "|    reward             | -1.5678602 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 145        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 84         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 70         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -222       |\n",
            "|    reward             | -3.1541252 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 61.5       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 85         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 76         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -1.04      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 156        |\n",
            "|    reward             | 0.16142894 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 14.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 85        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 82        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -121      |\n",
            "|    reward             | 1.9880885 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 11.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 87          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -26.9       |\n",
            "|    reward             | -0.53252256 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.931       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 85         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 93         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -28.9      |\n",
            "|    reward             | -1.8585756 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.835      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 85         |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 99         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -0.138     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 223        |\n",
            "|    reward             | -1.4577847 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 38.3       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 105         |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | -0.0136     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -5.96       |\n",
            "|    reward             | -0.63742095 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 12.9        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 85          |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 110         |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -42.3       |\n",
            "|    reward             | -0.02905229 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.59        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 85        |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 116       |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.0724   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -47.3     |\n",
            "|    reward             | -0.310616 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.22      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2022-04-04 to  2022-07-06\n",
            "a2c Sharpe Ratio:  -0.19158837577693624\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "day: 3083, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3711898.80\n",
            "total_reward: 2711898.80\n",
            "total_cost: 5949.44\n",
            "total_trades: 35746\n",
            "Sharpe: 0.666\n",
            "=================================\n",
            "======ddpg Validation from:  2022-04-04 to  2022-07-06\n",
            "ddpg Sharpe Ratio:  -0.18631176176848416\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "day: 3083, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3645368.50\n",
            "total_reward: 2645368.50\n",
            "total_cost: 1093.85\n",
            "total_trades: 43147\n",
            "Sharpe: 0.652\n",
            "=================================\n",
            "======td3 Validation from:  2022-04-04 to  2022-07-06\n",
            "td3 Sharpe Ratio:  -0.19449899372197946\n",
            "======sac Training========\n",
            "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "day: 3083, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4671852.18\n",
            "total_reward: 3671852.18\n",
            "total_cost: 293540.51\n",
            "total_trades: 70655\n",
            "Sharpe: 0.783\n",
            "=================================\n",
            "======sac Validation from:  2022-04-04 to  2022-07-06\n",
            "sac Sharpe Ratio:  -0.24593456144356982\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    fps             | 91       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 22       |\n",
            "|    total_timesteps | 2048     |\n",
            "| train/             |          |\n",
            "|    reward          | 0.669184 |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 89          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019080117 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0095     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.78        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0266     |\n",
            "|    reward               | -0.3201848  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 90          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014111072 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00563    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.7        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0184     |\n",
            "|    reward               | -10.038432  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 59.5        |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
        "                                                 PPO_model_kwargs,\n",
        "                                                 DDPG_model_kwargs,\n",
        "                                                 SAC_model_kwargs,\n",
        "                                                 TD3_model_kwargs,\n",
        "                                                 timesteps_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0qd8acMtj1f"
      },
      "outputs": [],
      "source": [
        "df_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4JKB--8tj1g"
      },
      "outputs": [],
      "source": [
        "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9mKF7GGtj1g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
        "\n",
        "df_account_value=pd.DataFrame()\n",
        "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
        "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
        "    df_account_value = df_account_value.append(temp,ignore_index=True)\n",
        "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
        "print('Sharpe Ratio: ',sharpe)\n",
        "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyosyW7_tj1g"
      },
      "outputs": [],
      "source": [
        "df_account_value.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLsRdw2Ctj1h"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df_account_value.account_value.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr2zX7ZxNyFQ"
      },
      "source": [
        "<a id='6.1'></a>\n",
        "## 7.1 BackTestStats\n",
        "pass in df_account_value, this information is stored in env class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzkr9yv-AdV_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
        "perf_stats_all = pd.DataFrame(perf_stats_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiHhM1YkoCel"
      },
      "outputs": [],
      "source": [
        "#baseline stats\n",
        "print(\"==============Get Baseline Stats===========\")\n",
        "df_dji_ = get_baseline(\n",
        "        ticker=\"^DJI\",\n",
        "        start = df_account_value.loc[0,'date'],\n",
        "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
        "\n",
        "stats = backtest_stats(df_dji_, value_col_name = 'close')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhJ9whD75WTs"
      },
      "outputs": [],
      "source": [
        "df_dji = pd.DataFrame()\n",
        "df_dji['date'] = df_account_value['date']\n",
        "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
        "print(\"df_dji: \", df_dji)\n",
        "df_dji.to_csv(\"df_dji.csv\")\n",
        "df_dji = df_dji.set_index(df_dji.columns[0])\n",
        "print(\"df_dji: \", df_dji)\n",
        "df_dji.to_csv(\"df_dji+.csv\")\n",
        "\n",
        "df_account_value.to_csv('df_account_value.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U6Suru3h1jc"
      },
      "source": [
        "<a id='6.2'></a>\n",
        "## 7.2 BackTestPlot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HggausPRoCem"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# print(\"==============Compare to DJIA===========\")\n",
        "# %matplotlib inline\n",
        "# # S&P 500: ^GSPC\n",
        "# # Dow Jones Index: ^DJI\n",
        "# # NASDAQ 100: ^NDX\n",
        "# backtest_plot(df_account_value,\n",
        "#               baseline_ticker = '^DJI',\n",
        "#               baseline_start = df_account_value.loc[0,'date'],\n",
        "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
        "df.to_csv(\"df.csv\")\n",
        "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
        "df_result_ensemble = df_result_ensemble.set_index('date')\n",
        "\n",
        "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
        "\n",
        "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
        "print(\"df_trade_date: \", df_trade_date)\n",
        "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
        "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
        "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
        "print(\"df_result_ensemble: \", df_result_ensemble)\n",
        "print(\"==============Compare to DJIA===========\")\n",
        "result = pd.DataFrame()\n",
        "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
        "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
        "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
        "print(\"result: \", result)\n",
        "result.to_csv(\"result.csv\")\n",
        "result.columns = ['ensemble', 'dji']\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "plt.figure();\n",
        "result.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBQx4bVQFi-a"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}